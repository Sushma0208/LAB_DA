{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtAducH6IDK7",
        "outputId": "c70035f7-b46d-4f7a-a130-8b24a2bbfebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import sys, subprocess\n",
        "\n",
        "def install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "except ImportError:\n",
        "    install(\"requests\")\n",
        "    import requests\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "except ImportError:\n",
        "    install(\"nltk\")\n",
        "    import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg_url = \"https://www.gutenberg.org/cache/epub/77428/pg77428.txt\"\n",
        "response = requests.get(gutenberg_url, timeout=30)\n",
        "\n",
        "if response.status_code != 200:\n",
        "    raise RuntimeError(f\"Failed to download: {response.status_code}\")\n",
        "\n",
        "raw_text = response.text\n"
      ],
      "metadata": {
        "id": "NaxfMMAIILo5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_marker = \"* START OF\"\n",
        "end_marker = \"* END OF\"\n",
        "start_idx = raw_text.find(start_marker)\n",
        "end_idx = raw_text.find(end_marker)\n",
        "if start_idx != -1 and end_idx != -1:\n",
        "    clean_text = raw_text[start_idx:end_idx]\n",
        "else:\n",
        "    clean_text = raw_text\n",
        "\n",
        "# Save locally\n",
        "with open(\"novel_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(clean_text)"
      ],
      "metadata": {
        "id": "45Cw0ACyIXQ8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "\n",
        "text_lower = clean_text.lower()\n",
        "text_no_punct = re.sub(r\"[^a-z0-9\\s]\", \" \", text_lower)\n",
        "text_clean = re.sub(r\"\\s+\", \" \", text_no_punct).strip()\n",
        "\n",
        "# Tokenization\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentences = sent_tokenize(clean_text)\n",
        "words = word_tokenize(text_clean)\n",
        "\n",
        "print(f\"Sentences: {len(sentences)}\")\n",
        "print(f\"Words: {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF2rxTJzIkML",
        "outputId": "7ea29a09-45fe-4031-d13c-caae2cea40e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: 6854\n",
            "Words: 80268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words_no_sw = [w for w in words if w not in stop_words and w.isalpha()]\n"
      ],
      "metadata": {
        "id": "Gs-dOZe3Jy_k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(words_no_sw)\n",
        "print(\"\\nTop 20 words (no stopwords):\")\n",
        "for word, freq in fdist.most_common(20):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKqlWmYaKU8Y",
        "outputId": "7daba028-aed9-4f38-846f-09ac36e9cff8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 words (no stopwords):\n",
            "one: 278\n",
            "conquest: 240\n",
            "would: 216\n",
            "tuan: 203\n",
            "garon: 169\n",
            "man: 153\n",
            "upon: 145\n",
            "like: 140\n",
            "know: 140\n",
            "black: 135\n",
            "could: 129\n",
            "muda: 129\n",
            "lhassa: 128\n",
            "white: 127\n",
            "eyes: 127\n",
            "seemed: 124\n",
            "night: 120\n",
            "said: 118\n",
            "yes: 117\n",
            "room: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(w) for w in words_no_sw]\n",
        "fdist_stems = FreqDist(stems)\n",
        "print(\"\\nTop 20 stems:\")\n",
        "for stem, freq in fdist_stems.most_common(20):\n",
        "    print(f\"{stem}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09seeM9VKYFp",
        "outputId": "8ee6fce6-9d34-48fb-ae9d-20ad94c76cb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 stems:\n",
            "one: 278\n",
            "conquest: 241\n",
            "would: 216\n",
            "tuan: 203\n",
            "garon: 169\n",
            "know: 164\n",
            "man: 154\n",
            "like: 151\n",
            "seem: 149\n",
            "upon: 145\n",
            "black: 140\n",
            "eye: 140\n",
            "go: 137\n",
            "smile: 137\n",
            "white: 129\n",
            "could: 129\n",
            "muda: 129\n",
            "lhassa: 128\n",
            "night: 127\n",
            "face: 127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the missing resource\n",
        "\n",
        "sample_tokens = words_no_sw[:500]\n",
        "pos_tags = nltk.pos_tag(sample_tokens)\n",
        "print(\"\\nSample POS tags (first 25):\")\n",
        "for token, tag in pos_tags[:25]:\n",
        "    print(f\"{token}: {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8DbOi5vLmhb",
        "outputId": "a888e97e-6b3b-400b-b8a2-c97473ecea21"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample POS tags (first 25):\n",
            "start: NN\n",
            "project: NN\n",
            "gutenberg: NN\n",
            "ebook: VB\n",
            "black: JJ\n",
            "parrot: NN\n",
            "black: JJ\n",
            "parrot: NN\n",
            "tale: NN\n",
            "golden: JJ\n",
            "chersonese: JJ\n",
            "harry: NN\n",
            "hervey: NN\n",
            "author: NN\n",
            "caravans: NNS\n",
            "night: NN\n",
            "etc: VBP\n",
            "perceive: JJ\n",
            "grace: NN\n",
            "romance: NN\n",
            "man: NN\n",
            "exalted: VBD\n",
            "animals: NNS\n",
            "james: NNS\n",
            "branch: VBP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sents = sentences[:10]\n",
        "tokenized_sents = [word_tokenize(s) for s in sample_sents]\n",
        "pos_tagged_sents = [nltk.pos_tag(ts) for ts in tokenized_sents]\n",
        "ner_chunks = [nltk.ne_chunk(tags) for tags in pos_tagged_sents]\n",
        "\n",
        "print(\"\\nNamed entities (sample):\")\n",
        "for tree in ner_chunks:\n",
        "    for subtree in tree:\n",
        "        if hasattr(subtree, \"label\") and subtree.label() in (\"PERSON\", \"ORGANIZATION\", \"GPE\"):\n",
        "            entity = \" \".join(token for token, _ in subtree.leaves())\n",
        "            print(f\"{subtree.label()}: {entity}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD4MFYl_LDSf",
        "outputId": "8b01de39-2729-4005-8862-f9b25b384c44"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named entities (sample):\n",
            "ORGANIZATION: THE\n",
            "ORGANIZATION: PROJECT\n",
            "ORGANIZATION: BLACK\n",
            "ORGANIZATION: PARROT\n",
            "ORGANIZATION: THE\n",
            "ORGANIZATION: BLACK\n",
            "ORGANIZATION: PARROT _A Tale\n",
            "ORGANIZATION: Golden Chersonese_\n",
            "ORGANIZATION: HARRY\n",
            "ORGANIZATION: ETC\n",
            "GPE: Romance\n",
            "ORGANIZATION: JAMES\n",
            "ORGANIZATION: BRANCH\n",
            "ORGANIZATION: CABELL\n",
            "ORGANIZATION: THE\n",
            "ORGANIZATION: CENTURY\n",
            "ORGANIZATION: BEDELL\n",
            "ORGANIZATION: Khmers\n",
            "PERSON: Manipur\n",
            "PERSON: Arakan\n",
            "ORGANIZATION: Lake\n",
            "PERSON: Tonle Sap\n",
            "GPE: Angkor\n",
            "ORGANIZATION: Tevadas\n",
            "PERSON: Naga\n",
            "ORGANIZATION: MAN\n",
            "ORGANIZATION: FROM\n",
            "ORGANIZATION: BLUE\n",
            "PERSON: _Cambodia_ V\n",
            "ORGANIZATION: CONQUEST\n",
            "ORGANIZATION: DREAM\n",
            "ORGANIZATION: MALAY\n",
            "ORGANIZATION: HOUSE\n",
            "ORGANIZATION: BARABBAS\n",
            "ORGANIZATION: BLACK\n",
            "ORGANIZATION: PARROT\n",
            "ORGANIZATION: MAN\n",
            "ORGANIZATION: FROM\n",
            "ORGANIZATION: Pacific\n",
            "GPE: Nowhere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "bigrams = list(ngrams(words_no_sw, 2))\n",
        "trigrams = list(ngrams(words_no_sw, 3))\n",
        "fdist_bi = FreqDist(bigrams)\n",
        "fdist_tri = FreqDist(trigrams)\n",
        "\n",
        "print(\"\\nTop 10 bigrams:\")\n",
        "for bg, freq in fdist_bi.most_common(10):\n",
        "    print(f\"{' '.join(bg)}: {freq}\")\n",
        "\n",
        "print(\"\\nTop 10 trigrams:\")\n",
        "for tg, freq in fdist_tri.most_common(10):\n",
        "    print(f\"{' '.join(tg)}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGP_YQ3mMhQ4",
        "outputId": "0ad26f34-4ece-49fd-fbbd-30b16c4485a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 bigrams:\n",
            "tuan muda: 129\n",
            "barth lemy: 100\n",
            "black parrot: 52\n",
            "abu hassan: 41\n",
            "tuan rajah: 40\n",
            "dr garth: 36\n",
            "blue slendong: 21\n",
            "emerald buddha: 20\n",
            "barabbas town: 19\n",
            "stephen conquest: 19\n",
            "\n",
            "Top 10 trigrams:\n",
            "captain barth lemy: 17\n",
            "mr da vargas: 15\n",
            "cap st jacques: 13\n",
            "le perroquet noir: 10\n",
            "pi noi bayadere: 8\n",
            "woman peacock shawl: 5\n",
            "man blue slendong: 4\n",
            "man scarred wrists: 4\n",
            "drew deep breath: 4\n",
            "black parrot know: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens_original = word_tokenize(clean_text)\n",
        "text_obj = nltk.Text(text_tokens_original)\n",
        "print(\"\\nConcordance for 'Guiana':\")\n",
        "try:\n",
        "    text_obj.concordance(\"Guiana\", width=80, lines=5)\n",
        "except Exception as e:\n",
        "    print(f\"No concordance output: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuY3VGpiM2Vi",
        "outputId": "2548a889-047b-4cf1-deac-cd484be0c858"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Concordance for 'Guiana':\n",
            "Displaying 5 of 19 matches:\n",
            "remember ? _ CONTENTS I THE MAN FROM GUIANA II EPISODE III THE BLUE SLENDONG IV \n",
            " BLACK PARROT CHAPTER I THE MAN FROM GUIANA He had come up from that necklace of\n",
            "iting . If so , he began near home ; Guiana 's just across from Hades , you know\n",
            "rchant of Hai Fong , who was sent to Guiana . The prisoners dubbed him the Black\n",
            "apes -- or evasions , as they say in Guiana . Then , one day , the very man who \n"
          ]
        }
      ]
    }
  ]
}